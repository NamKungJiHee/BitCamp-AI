1. SMOTE
SMOTE는 데이터 증폭기능으로 회귀가 아닌 분류모델에서 주로 사용한다.

================================================================

2. pickle
import pickle

* pickle 모듈을 이용하면 원하는 데이터를 자료형의 변경없이 파일로 저장하여 그대로 로드할 수 있음
* open('text', 'wb')방식으로 저장

pickle로 데이터를 저장하거나 불러올때는 파일을 바이트 형식으로 읽거나 써야함(wb, rb)

# 입력 #
pickle.dump(data, file)

# 로드 #
변수 = pickle.load(file)

ex) 
path = './_save/'                 # save
pickle.dump(model, open(path +'m23_pickle1_save.dat', 'wb'))

path = './_save/'                 # load
model = pickle.load(open(path + 'm23_pickle1_save.dat', 'rb')) 

================================================================

import joblib

ex)
path = './_save/'                 # save
joblib.dump(model, path + 'm24_joblib1_save.dat')

path = './_save/'                 # load
model = joblib.load(path + 'm24_joblib1_save.dat')

=================================================================

## Data 지정 방법 ##

1. Pandas
path = "../~"
datasets = pd.read_csv(path + 'winequality-white.csv', index_col = None, header = 0, sep=';')
x = datasets.drop(['quality','chlorides','pH'], axis = 1)
y = datasets['quality']
==============================================================
2. Numpy
datasets = pd.read_csv(path + 'winequality-white.csv', index_col = None, header = 0, sep=';') # 분리
datasets = datasets.values #####  pandas --> numpy로 바꿔주기 #####
#print(type(datasets)) # <class 'numpy.ndarray'>
x = datasets[:,:11]  # 모든 행, 10번째까지
y = datasets[:, 11]  # 모든행, 11번째 열이 y 
=============================================================
3. Numpy시
print("라벨: ", np.unique(y, return_counts = True))
return_counts = True하면 각각의 y값들이 무엇이 있고 몇개씩 있는지 알 수 있다.

4. Pandas시
pd.value_counts

###### pandas를 numpy로 바꾸려면 ######
a = a.values

===============================================================
# 그림 그리기
import matplotlib.pyplot as plt

def plot_feature_importances_dataset(model):
    n_features = x.data.shape[1]
    plt.barh(np.arange(n_features), model.feature_importances_,
             align = 'center')
    plt.yticks(np.arange(n_features), x.feature_names)
    plt.xlabel("Feature Importances")
    plt.ylabel("Feautures")
    plt.ylim(-1, n_features)
    
plt.subplot(2,2,1)  # 2행 2열로 뽑아내라 첫번째꺼를
plot_feature_importances_dataset(model1)
plt.subplot(2,2,2)
plot_feature_importances_dataset(model2)
plt.subplot(2,2,3)
plot_feature_importances_dataset(model3)
plt.subplot(2,2,4)
plot_feature_importances_dataset(model4)
plt.show()

===============================================================
log와 expm1
## 로그 변환 ##
y_train = np.log1p(y_train)
y_test = np.log1p(y_test)

## 지수로 다시 변환 ##
nmae = NMAE(np.expm1(y_test), np.expm1(y_predict)) 

========== 참고로 expm1은 (N,1)과 같은 행렬을 받아주지 않는다. 스칼라로 변환해야함 ==========

===============================================================
loc_iloc 차이

## loc / iloc 차이 ##
* loc 사용방법 *
df.loc[행 인덱싱 값, 열 인덱싱 값]
loc[ ]에 하나의 값만 입력한다면 그에 해당되는 하나의 행만 뽑아온다고 생각하면 된다.
Ex) df1.loc[0]을 입력한것은 'df1이라는 전체 데이터 프레임에서 인덱스 이름이 0인 행만 출력해서 가져와라'의 의미가 된다.
Ex) df1.loc[0,'Name'] 이라고 입력한다면 Name 칼럼의 0인행의 값을 출력해준다.
Ex) df1.loc[ : , : ]는 'df1라는 데이터 프레임에서 전체 행, 전체 열을 가져와라'의 의미다.
Ex) df1.loc[ : , 'Pclass'] 는 전체행을 추출하되 열은 내가 가져오고 싶은 특정 칼럼명
Ex) 특정 칼럼에서 값이 3인 값들만 추출하고 싶다면
     cond1 = df1['Pclass'] == 3
     df1.loc[cond1]

==================================================
# iloc는 index를 활용한 location 지정방법
* iloc 사용방법 *
df.iloc[행인덱스, 열인덱스]
Ex) df1.iloc[0,2] : 0번 행, 2번째 칼럼에 위치한 값
Ex) df1.iloc[::2, : ] : 데이터 프레임 df1에서 전체 값 중, 2간격으로 추출하고, 열은 전체 추출하라는 뜻!

===============================================================
# XGB
logloss : 이진분류
mlogloss : m은 multi 다중분류
rmse: 회귀모델

n_estimators : tensorflow에서 epochs값과 같다. 

XGBClassifier(n_estimators = 2000, 
              learning_rate =0.05, 
              n_jobs = -1, 
              max_depth = 5,         # max_depth = 소수점 x
              min_child_weight = 1,  # 가중치 합의 최소 / 값이 높을수록 과적합 방지
              subsample = 1,         # 학습에 사용하는 데이터 샘플링 비율(값이 낮을수록 과적합 방지)
              colsample_bytree = 1,  # 각 tree별 사용된 feature의 비율
              reg_alpha = 1,         # (가중치) 규제 L1 
              reg_lambda = 0,)       # 규제 L2 

==============================================================
eval_metric
모델의 평가 함수를 조정하는 함수

rmse: root mean square error
mae: mean absolute error
logloss: negative log-likelihood (이진분류)
error: Binary classification error rate (0.5 threshold)  (이진)
merror: Multiclass classification error rate  (다중)
mlogloss: Multiclass logloss  (다중)

==============================================================
for / if문
newlist = []
for i in y: 
    #print(i)
    if i <= 4:                   
        newlist += [0] 
    elif i <= 7:                
        newlist += [1]
    else:                      
        newlist += [2]

####################################
for i in range(len(y)):
    if y[i] < 4 : 
        y[i] = 4
    elif y[i] > 8:
        y[i] = 8

==============================================================
# feature_names

# Pandas
datasets = load_()
x = datasets.data
y = pd.DataFrame(x, columns = datasets['feature_names'])
y = datasets.target
#####################################
# Numpy
dataset = load_()  # return_X_y = True를 쓰면 바로 x와 y를 분리시켜준다.
# print(dataset.feature_names) # ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']

######################################
x, y = load_(return_X_y=True)
print(x.shape, y.shape)
# x = pd.DataFrame(x, columns=datasets['feature_names'])
# print(x.feature_names)
x = pd.DataFrame(x)
feature_names = ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']
x.columns = feature_names
print(x)
x = x.drop(['age', 'sex', 's1', 's4'], axis=1)
x = x.to_numpy()

==============================================================
# 디버깅 / 중복값처리
빨간점 찍고 F5키 누르기 --> python file 누르기  --> 한번 더 F5키 누르면 다음 지점까지 실행됨

중복값 처리 (duplicates)
datasets = datasets.drop_duplicates()

==============================================================
# 결측치 / 이상치

boosting은 결측치와 이상치로부터 자유롭다.

결측치 처리 방법
1) Drop
2) fillna로 결측치를 임의의 값으로 채우기(ex:0, 평균값, 중위값, 등등..)
3) interpolation(보간법) :  알려진 지점의 값 사이(중간)에 위치한 값을 알려진 값으로부터 추정하는 것
4) predict방식
(ex)
# Nan값을 y로 잡고 나머지를 x의 값으로 predict한다음 y_pred으로 나온 값을 그 nan 자리에 다시 넣는다!

===============================================================================
Series와 DataFrame의 차이
# Series = 벡터 // DataFrame = 행렬

### m19_bogan2.py = 결측치 채우는 방법들

================================================================================
이상치
ex1) -100,1,2,3,4,5,6,100000(10만)
중위값은 4(50%위치)
1사분위는 2 (4분의 1지점) / 3사분위는 6 (4분의 3지점)
1사분위에서 -150%(1.5) / 3사분위에서 +150%(1.5) = 여기에서 벗어난 값이 이상치
== *1.5(1.5를 곱해준다)

ex2)
-300, -200, -100, 2, 3, 4, 5, 100, 10만
중위값) 3
-100이 (4분의 1지점) 5가 (4분의 3지점)
5 -(-100) * 1.5 == 대략 150 이라고 가정.. 이값을 -100과 5의 지점에서 각각 더하거나 빼준다. 그 이외의 값들은 다 이상치로 생각

